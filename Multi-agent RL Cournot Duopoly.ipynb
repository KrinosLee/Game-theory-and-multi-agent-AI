{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NQVCToZkRV8"
      },
      "source": [
        "# Multi Agent Artificial Intelligence Practice - Cournot Duopoly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwvzHzFEp1Xv"
      },
      "source": [
        "Cournot Duopoly is a classic static game that models the imperfect competition in which multiple firms compete in price and production to capture market share.\n",
        "Since the firms' actions are continuous variables, the game is a continuous action setting.\n",
        "It is a **nonzero-sum game** (neither team-based nor zero-sum) which represents a challenge for current MARL methods.\n",
        "\n",
        "Let $a_i\\in [-A_i,A_i]$ represents the set of actions for agent $i\\in\\{1,2\\ldots, N\\}:=\\mathcal{N}$,\n",
        "where $A_i\\in \\mathbb{R}_{>0}$.\n",
        "Each agent $i$'s reward (profit) is \n",
        "$$\n",
        "R_i(a_i,a_{-i})=g_i(a_i,a_{-i})+ w_i(a_i),\n",
        "$$\n",
        "where\n",
        "$\n",
        "\\partial^{2} g_{i} / \\partial a_{i}^{2}<0, \\partial g_{i} / \\partial a_{-i}<0\n",
        "$,and \n",
        "$\\partial^{2} g_{i} / \\partial a_{i} \\partial a_{-i}<0\n",
        "$.\n",
        "Agents adopt Markov policies as\n",
        "$\n",
        "a_{i} = \\pi_i(a_{-i}).\n",
        "$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDLU7gSZ47Kq"
      },
      "source": [
        "#### TODO: Assume $N=2$, prove that policy $\\pi_i$ is non-increasing. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVy-ZoUs59rK"
      },
      "source": [
        "For Cournot duopoly we can express the reward function $R$ as:\n",
        "\n",
        "For agent 1:\n",
        "$R_1(a_1,a_2) = g_1(a_1,a_2)+ w_1(a_1)$\n",
        "\n",
        "For agent 2:\n",
        "$R_2(a_1,a_2) = g_2(a_1,a_2)+ w_2(a_2)$\n",
        "\n",
        "given $N = 2$, where $g_i$ is the action*price function and $w_i$ is the cost function.\n",
        "\n",
        "Since we are considering the case $N=2$, it is reasonable to assume the function $g_i = a_iP(a_i, a_{-i}) = a_i(k - a_i - a_{-i})$ and the cost function $w_i = -C_i(a_i)$ where $C_i(a_i)$ is an increasing function as when we increase the product quantity $a_i$ the cost for producing that much quantity will definitely increase. To confirm that $g_i$ satisfy the condition given:\n",
        "\n",
        "$g_i = a_i(k - a_i - a_{-i}) = ka_i - a_i^2 - a_ia_{-i}$\n",
        "\n",
        "$\\frac{\\partial^2 g_i}{\\partial a_i^2} = -2 < 0$\n",
        "\n",
        "$\\frac{\\partial^2 g_i}{\\partial a_i \\partial a_{-i}} = -1 < 0$\n",
        "\n",
        "$\\frac{\\partial g_i}{\\partial a_{-i}} = -a_i < 0$\n",
        "\n",
        "which satisfy the condition given. \n",
        "\n",
        "Now by considering the Nash Equilibrium of this case, we have:\n",
        "\n",
        "$R_i = ka_i - a_i^2 - a_ia_{-i} - C_i(a_i)$\n",
        "\n",
        "$\\frac{\\partial R_i}{\\partial a_i} = k - 2a_i - a_{-i} - \\frac{\\partial C_i(a_i)}{\\partial a_i}$\n",
        "\n",
        "$k - 2a_i - a_{-i} - \\frac{\\partial C_i(a_i)}{\\partial a_i} = 0$\n",
        "\n",
        "$a_i = \\frac{1}{2}\\left[k - a_{-i} - \\frac{\\partial C_i(a_i)}{\\partial a_i}\\right]$\n",
        "\n",
        "Then by symmetry:\n",
        "\n",
        "$a_{-i} = \\frac{1}{2}\\left[k - a_{i} - \\frac{\\partial C_{-i}(a_{-i})}{\\partial a_{-i}}\\right]$\n",
        "\n",
        "Combining $a_i, a_{-i}$ gives:\n",
        "\n",
        "$a_i = \\frac{1}{2}\\left[k - \\frac{1}{2}\\left[k - a_{i} - \\frac{\\partial C_{-i}(a_{-i})}{\\partial a_{-i}}\\right] - \\frac{\\partial C_i(a_i)}{\\partial a_i}\\right]$\n",
        "\n",
        "$a_i = \\frac{1}{3}\\left[k + \\frac{\\partial C_{-i}(a_{-i})}{\\partial a_{-i}} - 2\\frac{\\partial C_i(a_i)}{\\partial a_i} \\right]$\n",
        "\n",
        "Since the coefficient with $\\frac{\\partial C_i(a_i)}{\\partial a_i}$ is $-2$ and the coefficient with $\\frac{\\partial C_{-i}(a_{-i})}{\\partial a_{-i}}$ is $+1$ the term $\\left[k + \\frac{\\partial C_{-i}(a_{-i})}{\\partial a_{-i}} - 2\\frac{\\partial C_i(a_i)}{\\partial a_i} \\right]$ is definitely non increasing. Therefore $a_i = \\pi_i$ is non increasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX9ak9rd6HfZ"
      },
      "source": [
        "#### TODO: Set up Cournot Duopoly game.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s0VoU-V147c0"
      },
      "source": [
        "\n",
        "Suppose that \n",
        "$$\n",
        "g_i=a_i(\\alpha -\\beta\\sum_{j\\in\\mathcal{N}}a_j),\n",
        "w_i=\\gamma a_i.\n",
        "$$\n",
        "We choose $A_i=1.0, \\forall i\\in\\mathcal{N}$ and $\\alpha=1.5, \\beta=1.0, \\gamma=-0.5$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "r_2R3spG77Do"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import gym\n",
        "\n",
        "class CournotDuopoly(gym.Env):\n",
        "    def __init__(self, agent_num=2, action_range=(-1., 1.)):\n",
        "        self.agent_num = agent_num\n",
        "        self.action_space = gym.spaces.Box(low=-1.0, high=1.0, shape=(1,))\n",
        "        self.rewards = np.zeros((self.agent_num,))\n",
        "        self.t = 0\n",
        "\n",
        "        alpha = 1.5\n",
        "        beta = 1.0\n",
        "        gamma = -0.5\n",
        "\n",
        "        def payoff_n_cournot(action_n, i):\n",
        "            \"\"\"\n",
        "            Define the payoff function R_i(a_i,a_{-i}).\n",
        "            :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
        "            :param i: agent index\n",
        "            :return: R_i(a_i,a_{-i})\n",
        "            \"\"\"\n",
        "            ########### TODO: Compute R_i(a_i,a_{-i}) (1 point) ###########\n",
        "            r = action_n[i]*(alpha - beta*np.sum(action_n)) + gamma*action_n[i]\n",
        "            ########### END TODO ############################\n",
        "            return r\n",
        "\n",
        "        def payoff_n_cournot_derivative(action_n, i):\n",
        "            \"\"\"\n",
        "            Define the partial derivative of the payoff function R_i(a_i,a_{-i}) w.r.t. a_i.\n",
        "            :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
        "            :param i: agent index\n",
        "            :return: \\partial R_i(a_i,a_{-i}) / \\partial a_i\n",
        "            \"\"\"\n",
        "            ########### TODO: Compute \\partial R_i(a_i,a_{-i}) / \\partial a_i (1 point) ###########\n",
        "            dr = alpha + gamma - beta*np.sum(action_n)\n",
        "            ########### END TODO ############################\n",
        "            return dr\n",
        "        \n",
        "        self.payoff = payoff_n_cournot\n",
        "        self.payoff_n_derivative = payoff_n_cournot_derivative\n",
        "\n",
        "    def step(self, action_n):\n",
        "        \"\"\"\n",
        "        Define the environment step function.\n",
        "        :param action_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
        "        :return: state_n: (nd.array) a list of all agents' actions, shape is (agent_num,)\n",
        "        :return: reward_n: (nd.array) a list of all agents' states, shape is (agent_num,)\n",
        "        :return: done_n: (nd.array) a list of all agents' done status, shape is (agent_num,)\n",
        "        :return: info: (dict) a dictionary of customized information\n",
        "        \"\"\"\n",
        "        actions = np.array(action_n).reshape((self.agent_num,))\n",
        "        reward_n = np.zeros((self.agent_num,))\n",
        "        payoff_derivative_n = np.zeros((self.agent_num,))\n",
        "        for i in range(self.agent_num):\n",
        "            payoff_derivative_n[i] = self.payoff_n_derivative(actions, i)\n",
        "            reward_n[i] = self.payoff(actions, i)\n",
        "        self.rewards = reward_n\n",
        "        state_n = np.array(list([[0.0 * i] for i in range(self.agent_num)]))\n",
        "        info = {'reward_n': reward_n, 'reward_n_derivative': payoff_derivative_n} \n",
        "        done_n = np.array([True] * self.agent_num)\n",
        "        self.t += 1\n",
        "        # print(\"state_n, reward_n, done_n, info\", state_n, reward_n, done_n, info)\n",
        "        return state_n, reward_n, done_n, info\n",
        "\n",
        "    def reset(self):\n",
        "        return np.array(list([[0.0 * i] for i in range(self.agent_num)]))\n",
        "\n",
        "    def get_rewards(self):\n",
        "        return self.rewards\n",
        "    \n",
        "    def render(self, mode=\"human\", close=False):\n",
        "        pass\n",
        "\n",
        "    def terminate(self):\n",
        "        pass\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcYFl6ZLIxn1"
      },
      "source": [
        "#### TODO: Implement MADDPG agents to play the Cournot Duopoly Game. (3 points)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrXdbTolJ0QQ"
      },
      "source": [
        "Implement the MADDPG algorithm presented in the paper:\n",
        "[Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments](https://arxiv.org/pdf/1706.02275.pdf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "9nsdLcEdLFJS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "\n",
        "class Actor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Actor, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.action_out = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        actions = torch.tanh(self.action_out(x))\n",
        "        return actions\n",
        "\n",
        "\n",
        "class Critic(nn.Module):\n",
        "    def __init__(self, obs_shape, action_shape):\n",
        "        super(Critic, self).__init__()\n",
        "        self.fc1 = nn.Linear(obs_shape + action_shape, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 64)\n",
        "        self.q_out = nn.Linear(64, 1)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        state = torch.cat(state, dim=1)\n",
        "        action = torch.cat(action, dim=1)\n",
        "        x = torch.cat([state, action], dim=1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = F.relu(self.fc3(x))\n",
        "        q_value = self.q_out(x)\n",
        "        return q_value\n",
        "\n",
        "\n",
        "class MADDPG:\n",
        "    def __init__(self, n_agents, agent_id, obs_shape=1, action_shape=1):\n",
        "        self.n_agents = n_agents\n",
        "        self.agent_id = agent_id\n",
        "        self.action_shape = action_shape\n",
        "        self.train_step = 0\n",
        "        self.lr_actor = 1e-4\n",
        "        self.lr_critic = 1e-3\n",
        "        self.tau = 0.01\n",
        "        self.gamma = 0.95\n",
        "\n",
        "        # create the network\n",
        "        self.actor_network = Actor()\n",
        "        self.critic_network = Critic(obs_shape * self.n_agents,\n",
        "                                     action_shape * self.n_agents)\n",
        "\n",
        "        # build up the target network\n",
        "        self.actor_target_network = Actor()\n",
        "        self.critic_target_network = Critic(obs_shape * self.n_agents,\n",
        "                                            action_shape * self.n_agents)\n",
        "\n",
        "        # load the weights into the target networks\n",
        "        self.actor_target_network.load_state_dict(self.actor_network.state_dict())\n",
        "        self.critic_target_network.load_state_dict(self.critic_network.state_dict())\n",
        "\n",
        "        # create the optimizer\n",
        "        self.actor_optim = torch.optim.Adam(self.actor_network.parameters(), lr=self.lr_actor)\n",
        "        self.critic_optim = torch.optim.Adam(self.critic_network.parameters(), lr=self.lr_critic)\n",
        "\n",
        "    # soft update\n",
        "    def _soft_update_target_network(self):\n",
        "        for target_param, param in zip(self.actor_target_network.parameters(),\n",
        "                                       self.actor_network.parameters()):\n",
        "            ########### TODO: Soft-update target actor network (0.5 point) ###########\n",
        "            target_param.data.copy_(self.tau*param.data + (1 - self.tau)*target_param.data)\n",
        "            ########### END TODO ############################\n",
        "\n",
        "        for target_param, param in zip(self.critic_target_network.parameters(),\n",
        "                                       self.critic_network.parameters()):\n",
        "            ########### TODO: Soft-update target critic network (0.5 point) ###########\n",
        "            target_param.data.copy_(self.tau*param.data + (1 - self.tau)*target_param.data)\n",
        "            ########### END TODO ############################\n",
        "\n",
        "    # update the network\n",
        "    def train(self, transitions, other_agents):\n",
        "        for key in transitions.keys():\n",
        "            transitions[key] = torch.tensor(transitions[key], dtype=torch.float32)\n",
        "        r = transitions['r_%d' % self.agent_id]\n",
        "        o, u, o_next = [], [], []\n",
        "        for agent_id in range(self.n_agents):\n",
        "            o.append(transitions['o_%d' % agent_id])\n",
        "            u.append(transitions['u_%d' % agent_id])\n",
        "            o_next.append(transitions['o_next_%d' % agent_id])\n",
        "\n",
        "        # calculate the target Q value function\n",
        "        u_next = []\n",
        "        with torch.no_grad():\n",
        "            index = 0\n",
        "            for agent_id in range(self.n_agents):\n",
        "                if agent_id == self.agent_id:\n",
        "                    u_next.append(self.actor_target_network(o_next[agent_id]))\n",
        "                else:\n",
        "                    u_next.append(other_agents[index].actor_target_network(o_next[agent_id]))\n",
        "                    index += 1\n",
        "            q_next = self.critic_target_network(o_next, u_next).detach()\n",
        "\n",
        "            ########### TODO: Calculate the target Q value function (0.5 point) ###########\n",
        "            target_q = r.reshape(r.size()[0], 1) + self.gamma*self.critic_network.forward(o_next, u_next)\n",
        "            ########### END TODO ############################\n",
        "\n",
        "        # the q loss\n",
        "        q_value = self.critic_network(o, u)\n",
        "        ########### TODO: Calculate the critic loss (0.5 point) ###########\n",
        "        critic_loss = ((target_q - q_value)**2).mean()\n",
        "        ########### END TODO ############################\n",
        "\n",
        "        # the actor loss\n",
        "        u[self.agent_id] = self.actor_network(o[self.agent_id])\n",
        "        ########### TODO: Calculate the actor loss (0.5 point) ###########\n",
        "        actor_loss = -self.critic_network.forward(o, u).mean()\n",
        "        ########### END TODO ############################\n",
        "        # update the network\n",
        "        self.actor_optim.zero_grad()\n",
        "        actor_loss.backward()\n",
        "        self.actor_optim.step()\n",
        "        self.critic_optim.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optim.step()\n",
        "\n",
        "        self._soft_update_target_network()\n",
        "        self.train_step += 1\n",
        "\n",
        "    def select_action(self, o, noise_rate, epsilon):\n",
        "        if np.random.uniform() < epsilon:\n",
        "            u = np.random.uniform(-1.0, 1.0, self.action_shape)\n",
        "        else:\n",
        "            inputs = torch.tensor(o, dtype=torch.float32).unsqueeze(0)\n",
        "            ########### TODO: Take action based on the actor network (0.5 point) ###########\n",
        "            u = self.actor_network.forward(inputs)[0].detach().numpy() + noise_rate\n",
        "            ########### END TODO ############################\n",
        "        return u.copy()\n",
        "\n",
        "    def learn(self, transitions, other_agents):\n",
        "        self.train(transitions, other_agents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43hOXeEOPRwT"
      },
      "source": [
        "#### Some useful scripts (please execute).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bXm7HQCsIqQF"
      },
      "outputs": [],
      "source": [
        "import threading\n",
        "\n",
        "class Buffer:\n",
        "    def __init__(self, n_agents=5):\n",
        "        self.n_agents = n_agents\n",
        "        self.size = int(5e5)\n",
        "        # memory management\n",
        "        self.current_size = 0\n",
        "        # create the buffer to store info\n",
        "        self.buffer = dict()\n",
        "        for i in range(self.n_agents):\n",
        "            self.buffer['o_%d' % i] = np.empty([self.size, 1])\n",
        "            self.buffer['u_%d' % i] = np.empty([self.size, 1])\n",
        "            self.buffer['r_%d' % i] = np.empty([self.size])\n",
        "            self.buffer['o_next_%d' % i] = np.empty([self.size, 1])\n",
        "        # thread lock\n",
        "        self.lock = threading.Lock()\n",
        "\n",
        "    # store the episode\n",
        "    def store_episode(self, o, u, r, o_next):\n",
        "        idxs = self._get_storage_idx(inc=1)\n",
        "        for i in range(self.n_agents):\n",
        "            with self.lock:\n",
        "                self.buffer['o_%d' % i][idxs] = o[i]\n",
        "                self.buffer['u_%d' % i][idxs] = u[i]\n",
        "                self.buffer['r_%d' % i][idxs] = r[i]\n",
        "                self.buffer['o_next_%d' % i][idxs] = o_next[i]\n",
        "    \n",
        "    # sample the data from the replay buffer\n",
        "    def sample(self, batch_size):\n",
        "        temp_buffer = {}\n",
        "        idx = np.random.randint(0, self.current_size, batch_size)\n",
        "        for key in self.buffer.keys():\n",
        "            temp_buffer[key] = self.buffer[key][idx]\n",
        "        return temp_buffer\n",
        "\n",
        "    def _get_storage_idx(self, inc=None):\n",
        "        inc = inc or 1\n",
        "        if self.current_size+inc <= self.size:\n",
        "            idx = np.arange(self.current_size, self.current_size+inc)\n",
        "        elif self.current_size < self.size:\n",
        "            overflow = inc - (self.size - self.current_size)\n",
        "            idx_a = np.arange(self.current_size, self.size)\n",
        "            idx_b = np.random.randint(0, self.current_size, overflow)\n",
        "            idx = np.concatenate([idx_a, idx_b])\n",
        "        else:\n",
        "            idx = np.random.randint(0, self.size, inc)\n",
        "        self.current_size = min(self.size, self.current_size+inc)\n",
        "        if inc == 1:\n",
        "            idx = idx[0]\n",
        "        return idx\n",
        "\n",
        "def evaluate(env, agents, agent_num, evaluate_episodes, evaluate_episode_len):\n",
        "    returns = []\n",
        "    for episode in range(evaluate_episodes):\n",
        "        # reset the environment\n",
        "        s = env.reset()\n",
        "        rewards_n = np.zeros(agent_num)\n",
        "        rs = []\n",
        "        alist = []\n",
        "        rewards1 = 0\n",
        "        for time_step in range(evaluate_episode_len):\n",
        "            actions = []\n",
        "            with torch.no_grad():\n",
        "                for agent_id, agent in enumerate(agents):\n",
        "                    action = agent.select_action(s[agent_id], 0, 0)\n",
        "                    actions.append(action)\n",
        "            s_next, r, done, info = env.step(actions)\n",
        "            if type(info['reward_n']) is list:\n",
        "                rewards_n += np.sum(info['reward_n'])\n",
        "            else:\n",
        "                rewards_n += info['reward_n'].squeeze()\n",
        "\n",
        "            s = s_next\n",
        "        returns.append(rewards_n)\n",
        "    s = env.reset()\n",
        "    mean_return = sum(returns) / evaluate_episodes\n",
        "\n",
        "    return mean_return\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Et0OSYA-Ur48"
      },
      "source": [
        "#### Test your implemented MADDPG agent in the Cournot Duopoly Game.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "QKBz8uINUoL9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  1%|          | 243/20000 [00:08<06:30, 50.65it/s] b:\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "100%|██████████| 20000/20000 [16:05<00:00, 20.71it/s]  \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4.61697491 4.26296117 5.29043831 5.64144299 4.76224635]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "import seaborn as sns; sns.set()\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
        "\n",
        "\n",
        "noise = 0.1\n",
        "epsilon = 0.1\n",
        "episode_limit = 100\n",
        "n_agents = 5\n",
        "batch_size = 256\n",
        "evaluate_rate = 1000\n",
        "time_steps = 20000\n",
        "evaluate_episode_len = 100\n",
        "evaluate_episodes = 100\n",
        "env = CournotDuopoly(agent_num=n_agents)\n",
        "agents = [MADDPG(n_agents, i, obs_shape=1, action_shape=1) for \n",
        "          i in range(n_agents)]\n",
        "buffer = Buffer(n_agents=n_agents)\n",
        "\n",
        "returns = []\n",
        "done = None\n",
        "mean_return_eval = 0.\n",
        "for time_step in tqdm(range(time_steps)):\n",
        "    if time_step % episode_limit == 0 or np.all(done):\n",
        "        s = env.reset()\n",
        "    u = []\n",
        "    actions = []\n",
        "    with torch.no_grad():\n",
        "        for agent_id, agent in enumerate(agents):\n",
        "            action = agent.select_action(s[agent_id], noise, epsilon)\n",
        "            u.append(action)\n",
        "            actions.append(action)\n",
        "\n",
        "    s_next, r, done, info = env.step(actions)\n",
        "    buffer.store_episode(s[:n_agents], u,\n",
        "                         r[:n_agents], s_next[:n_agents])\n",
        "\n",
        "    s = s_next\n",
        "\n",
        "    if buffer.current_size >= batch_size:\n",
        "        transitions = buffer.sample(batch_size)\n",
        "        for agent in agents:\n",
        "            other_agents = agents.copy()\n",
        "            other_agents.remove(agent)\n",
        "            agent.learn(transitions, other_agents)\n",
        "\n",
        "    if time_step == 0 or time_step % evaluate_rate == 0:\n",
        "        mean_return_eval = evaluate(env, agents, n_agents,\n",
        "                                    evaluate_episodes, evaluate_episode_len)\n",
        "        returns.append(mean_return_eval)\n",
        "print(mean_return_eval)\n",
        "plt.figure()\n",
        "plt.plot(range(len(returns)), np.array(returns).sum(-1))\n",
        "plt.xlabel('episode * ' + str(evaluate_rate / episode_limit))\n",
        "plt.ylabel('average returns')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "43hOXeEOPRwT"
      ],
      "name": "COMP0124_MAAI_2021_Individual_CW_Public.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3.7.10 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "4cc37d27a82e88f770e2d4fe0766c3939bb8f78b89c7c20c09b2b06a9fd1633f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
